Additional & Related Reading Materials

(1) Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision (2023)
- Link: https://www.semanticscholar.org/paper/Principle-Driven-Self-Alignment-of-Language-Models-Sun-Shen/e01515c6138bc525f7aec30fc85f2adf028d4156
- Summary: An AI assistant is developed that combines principle-driven reasoning and the generative abilities of LLMs allowing for self-alignment and regulation of AI tools with minimal human supervision.

RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback (2023)
https://www.semanticscholar.org/paper/RLAIF-vs.-RLHF%3A-Scaling-Reinforcement-Learning-from-Lee-Phatale/600ff4c4ae9fc506c86673c5ecce4fa90803e987

Summary: RLAIF (also used in Constituional AI) achieves comparable performance to RLHF, suggesting that training and tuning AI models may require less human participation than previously understood, allowing for larger scalability.

